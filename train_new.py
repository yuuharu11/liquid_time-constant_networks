#!/usr/bin/env python3
"""
æ©Ÿæ¢°å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ä½¿ç”¨æ–¹æ³•:
    python train.py                    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã§å®Ÿè¡Œ
    python train.py dataset=cifar      # ç•°ãªã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨
    python train.py model=resnet       # ç•°ãªã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
"""

import os
import warnings
import time
from pathlib import Path

import hydra
import pytorch_lightning as pl
import torch
from omegaconf import DictConfig, OmegaConf
from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping
from pytorch_lightning.loggers import WandbLogger
from rich import print

# ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã‚·ã‚¹ãƒ†ãƒ ã¨ utilities
import src.utils as utils
from src.utils import registry

# è­¦å‘Šã‚’æŠ‘åˆ¶ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
warnings.filterwarnings("ignore", category=UserWarning)

# TensorFloat32ã‚’æœ‰åŠ¹åŒ–ï¼ˆå¤§ããªãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã‚’é«˜é€ŸåŒ–ï¼‰
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True


def setup_logger(cfg: DictConfig) -> WandbLogger:
    """Weights & Biases ãƒ­ã‚¬ãƒ¼ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
    logger = WandbLogger(
        project="sequence_models",
        name=f"experiment-{cfg.model.get('_name_', 'unknown')}",
        tags=[cfg.dataset.get('_name_', 'unknown')],
        offline=True,  # é–‹ç™ºä¸­ã¯ã‚ªãƒ•ãƒ©ã‚¤ãƒ³
    )
    return logger


def setup_callbacks(cfg: DictConfig) -> list:
    """PyTorch Lightning ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ï¼ˆãƒ¬ã‚¸ã‚¹ãƒˆãƒªçµŒç”±ï¼‰"""
    callbacks = []
    
    # ãƒ¢ãƒ‡ãƒ«ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
    checkpoint_config = DictConfig({
        '_target_': 'src.callbacks.model_checkpoint.ModelCheckpoint',
        'monitor': 'val_loss',
        'mode': 'min',
        'save_top_k': 3,
        'filename': '{epoch:02d}-{val_loss:.3f}',
        'save_last': True,
        'verbose': True,
    })
    callbacks.append(utils.instantiate(registry.callbacks, checkpoint_config))
    
    # Early stopping
    early_stop_config = DictConfig({
        '_target_': 'src.callbacks.early_stopping.EarlyStopping',
        'monitor': 'val_loss',
        'patience': 10,
        'mode': 'min',
        'verbose': True,
    })
    callbacks.append(utils.instantiate(registry.callbacks, early_stop_config))
    
    # Learning rate monitor
    lr_monitor_config = DictConfig({
        '_target_': 'src.callbacks.learning_rate_monitor.LearningRateMonitor',
        'logging_interval': 'epoch',
    })
    callbacks.append(utils.instantiate(registry.callbacks, lr_monitor_config))
    
    return callbacks


def create_dataloader(cfg: DictConfig):
    """ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’ä½œæˆ"""
    print(f"[cyan]ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™: {cfg.dataset._name_}[/cyan]")
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã‹ã‚‰ä½œæˆ
    dataset = utils.instantiate(registry.dataset, cfg.dataset)
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼è¨­å®š
    loader_config = cfg.loader
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’ä½œæˆ
    train_loader = dataset.train_dataloader(
        batch_size=loader_config.batch_size,
        shuffle=True,
        num_workers=loader_config.get('num_workers', 4),
    )
    
    val_loader = dataset.val_dataloader(
        batch_size=loader_config.batch_size,
        shuffle=False,
        num_workers=loader_config.get('num_workers', 4),
    )
    
    test_loader = dataset.test_dataloader(
        batch_size=loader_config.batch_size,
        shuffle=False,
        num_workers=loader_config.get('num_workers', 4),
    )
    
    print(f"   âœ… ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ä½œæˆå®Œäº†")
    print(f"   ğŸ“ˆ è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(train_loader)} ãƒãƒƒãƒ")
    print(f"   ğŸ“Š æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {len(val_loader)} ãƒãƒƒãƒ")
    print(f"   ğŸ§ª ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len(test_loader)} ãƒãƒƒãƒ")
    
    return train_loader, val_loader, test_loader, dataset


def create_task_and_model(cfg: DictConfig, dataset):
    """ã‚¿ã‚¹ã‚¯ã¨ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ"""
    print(f"[green]ğŸ—ï¸  ãƒ¢ãƒ‡ãƒ«ã¨ã‚¿ã‚¹ã‚¯ä½œæˆ[/green]")
    
    # ãƒ¢ãƒ‡ãƒ«ã®è¨­å®šã‚’æº–å‚™
    model_config = cfg.model.copy()
    model_config.d_input = dataset.d_input
    
    # ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ
    model = utils.instantiate(registry.model, model_config)
    print(f"   âœ… ãƒ¢ãƒ‡ãƒ«ä½œæˆ: {model.__class__.__name__}")
    
    # ã‚¿ã‚¹ã‚¯ã®è¨­å®šã‚’æº–å‚™
    task_config = cfg.task.copy()
    task_config.model = model_config  # ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚’ã‚¿ã‚¹ã‚¯ã«æ¸¡ã™
    task_config.d_output = dataset.d_output
    task_config.optimizer = cfg.optimizer
    task_config.scheduler = cfg.get('scheduler', None)
    
    # ã‚¿ã‚¹ã‚¯ã‚’ä½œæˆ
    task = utils.instantiate(registry.task, task_config)
    
    # ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¿ã‚¹ã‚¯ã«è¨­å®š
    task.set_model(model)
    
    print(f"   âœ… ã‚¿ã‚¹ã‚¯ä½œæˆ: {task.__class__.__name__}")
    print(f"   ğŸ“Š å…¥åŠ›æ¬¡å…ƒ: {dataset.d_input}")
    print(f"   ğŸ¯ å‡ºåŠ›æ¬¡å…ƒ: {dataset.d_output}")
    
    # ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’è¡¨ç¤º
    if hasattr(model, 'get_info'):
        info = model.get_info()
        print(f"   ğŸ“Š ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {info.get('parameters', 'N/A'):,}")
    
    return task


@hydra.main(version_base=None, config_path="config", config_name="config")
def main(cfg: DictConfig) -> None:
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    print("[bold blue]ğŸš€ æ©Ÿæ¢°å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³é–‹å§‹[/bold blue]")
    print(f"âš™ï¸  è¨­å®š: {OmegaConf.to_yaml(cfg)}")
    
    # ã‚·ãƒ¼ãƒ‰ã®è¨­å®š
    if hasattr(cfg, 'seed'):
        pl.seed_everything(cfg.seed, workers=True)
        print(f"ğŸŒ± ã‚·ãƒ¼ãƒ‰è¨­å®š: {cfg.seed}")
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®ä½œæˆ
    train_loader, val_loader, test_loader, dataset = create_dataloader(cfg)
    
    # ã‚¿ã‚¹ã‚¯ã¨ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ
    task = create_task_and_model(cfg, dataset)
    
    # ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¨ãƒ­ã‚¬ãƒ¼ã®è¨­å®š
    logger = setup_logger(cfg)
    callbacks = setup_callbacks(cfg)
    
    # ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã®è¨­å®š
    trainer_config = cfg.trainer
    trainer = pl.Trainer(
        max_epochs=trainer_config.get('max_epochs', 100),
        accelerator=trainer_config.get('accelerator', 'auto'),
        devices=trainer_config.get('devices', 'auto'),
        precision=trainer_config.get('precision', 32),
        logger=logger,
        callbacks=callbacks,
        enable_checkpointing=True,
        enable_model_summary=True,
        enable_progress_bar=True,
        deterministic=cfg.get('deterministic', False),
    )
    
    print(f"[green]ğŸƒâ€â™‚ï¸ è¨“ç·´é–‹å§‹[/green]")
    
    # è¨“ç·´ã®å®Ÿè¡Œ
    start_time = time.time()
    trainer.fit(task, train_loader, val_loader)
    training_time = time.time() - start_time
    
    print(f"[green]âœ… è¨“ç·´å®Œäº† (æ™‚é–“: {training_time:.2f}s)[/green]")
    
    # ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ
    if test_loader is not None:
        print(f"[blue]ğŸ§ª ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ[/blue]")
        test_result = trainer.test(task, test_loader)
        print(f"[blue]âœ… ãƒ†ã‚¹ãƒˆå®Œäº†[/blue]")
        print(f"ğŸ“Š ãƒ†ã‚¹ãƒˆçµæœ: {test_result}")
    
    print("[bold green]ğŸ‰ ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Œäº†![/bold green]")


if __name__ == "__main__":
    main()
