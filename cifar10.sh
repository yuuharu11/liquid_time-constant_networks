#!/bin/bash
set -e

# --- å®Ÿé¨“ã®åŸºæœ¬è¨­å®š ---
BASE_EXPERIMENT="ltc_ncps/cifar10"
WANDB_PROJECT="ncps_cifar10"

# --- å¤‰æ›´ã—ã¦è©¦ã—ãŸã„ `units` ã®ãƒªã‚¹ãƒˆã‚’å®šç¾© ---
UNITS=256

# --- æœ€é©ã ã£ãŸãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å›ºå®š ---
D_MODEL=128
OUTPUT_UNITS=10
ODE_UNFOLDS=(2 3)
lr=5e-3
N_LAYERS=1

# --- `units` ã”ã¨ã«ãƒ«ãƒ¼ãƒ—ã‚’å®Ÿè¡Œ ---
for ode_unfolds in "${ODE_UNFOLDS[@]}"; do

  # --- å„å®Ÿé¨“ã§ã€ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªåå‰ã¨å‡ºåŠ›å…ˆã‚’ç”Ÿæˆ ---
  RUN_NAME="ode_unfolds${ode_unfolds}_conv2d"
  OUTPUT_DIR="outputs/ncps/cifar10/${RUN_NAME}"

  echo ""
  echo "=================================================================="
  echo "--- Running experiment: ${RUN_NAME} ---"
  echo "=================================================================="

  # å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰
  python3 train.py \
      experiment=$BASE_EXPERIMENT \
      model.n_layers=$N_LAYERS \
      model.d_model=$D_MODEL \
      model.layer.units.1.units=$UNITS \
      model.layer.units.2.output_units=$OUTPUT_UNITS \
      model.layer.ode_unfolds=$ode_unfolds \
      optimizer.lr=$lr \
      loader.batch_size=128 \
      trainer.max_epochs=150 \
      train.test=true \
      hydra.run.dir=$OUTPUT_DIR \
      wandb.project=$WANDB_PROJECT \
      wandb.name=$RUN_NAME
      
done

echo ""
echo "=================================================================="
echo "--- ğŸ“œ All final sweep experiments finished! ---"
echo "=================================================================="