# @package _global_
train:
  interval: epoch

scheduler:
  # _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
  _name_: plateau
  mode: "min" # Which metric to monitor
  monitor: "train/loss"
  factor: 0.2  # Decay factor when ReduceLROnPlateau is used
  patience: 10
  min_lr: 0.0  # Minimum learning rate during annealing
