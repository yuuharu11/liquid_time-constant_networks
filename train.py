#!/usr/bin/env python3
"""
æ©Ÿæ¢°å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ä½¿ç”¨æ–¹æ³•:
    python train.py                    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã§å®Ÿè¡Œ
    python train.py dataset=cifar      # ç•°ãªã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨
    python train.py model=resnet       # ç•°ãªã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
"""

import os
import warnings
import time
from pathlib import Path

import hydra
import pytorch_lightning as pl
import torch
from omegaconf import DictConfig, OmegaConf
from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping
from pytorch_lightning.loggers import WandbLogger
from rich import print

# ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã‚·ã‚¹ãƒ†ãƒ ã¨ utilities
import src.utils as utils
from src.utils import registry

# è­¦å‘Šã‚’æŠ‘åˆ¶ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
warnings.filterwarnings("ignore", category=UserWarning)

# TensorFloat32ã‚’æœ‰åŠ¹åŒ–ï¼ˆå¤§ããªãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã‚’é«˜é€ŸåŒ–ï¼‰
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True


class SequenceLightningModule(pl.LightningModule):
    """
    ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãƒ¢ãƒ‡ãƒ«ç”¨ã®PyTorch Lightningãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
    å…ƒã®ãƒªãƒã‚¸ãƒˆãƒªã®æ§‹é€ ã‚’å‚è€ƒã«ã€ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã‚·ã‚¹ãƒ†ãƒ ã‚’ä½¿ç”¨
    """
    def __init__(self, config):
        super().__init__()
        # è¨­å®šã‚’ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ã—ã¦ä¿å­˜
        self.save_hyperparameters(config, logger=False)
        
        # ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ãƒ•ãƒ©ã‚°ï¼ˆé‡è¤‡ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚’é˜²ãï¼‰
        self._has_setup = False
        
        # çŠ¶æ…‹ç®¡ç†ã®åˆæœŸåŒ–
        self._state = None
        
        print("[yellow]ğŸ“¦ SequenceLightningModuleåˆæœŸåŒ–å®Œäº†[/yellow]")
        
    def setup(self, stage=None):
        """ãƒ¢ãƒ‡ãƒ«ã¨ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        if self._has_setup:
            return
        else:
            self._has_setup = True
            
        print(f"[green]ğŸ”§ ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—é–‹å§‹ (stage: {stage})[/green]")
        
        # ãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ï¼ˆãƒ¬ã‚¸ã‚¹ãƒˆãƒªçµŒç”±ï¼‰
        print("ğŸ—ï¸  ãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–...")
        self.model = utils.instantiate(registry.model, self.hparams.model)
        print(f"   âœ… ãƒ¢ãƒ‡ãƒ«ä½œæˆ: {self.model.__class__.__name__}")
        
        # ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã®è¡¨ç¤º
        if hasattr(self.model, 'get_info'):
            info = self.model.get_info()
            print(f"   ğŸ“Š ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {info.get('parameters', 'N/A'):,}")
        
        print(f"[green]âœ… ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†[/green]")
        
    def configure_optimizers(self):
        """ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã¨ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã®è¨­å®šï¼ˆãƒ¬ã‚¸ã‚¹ãƒˆãƒªçµŒç”±ï¼‰"""
        print("[blue]ğŸ“Š ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼è¨­å®š[/blue]")
        
        # ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å–å¾—
        params = list(self.model.parameters())
        print(f"   ğŸ“ æœ€é©åŒ–å¯¾è±¡ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {sum(p.numel() for p in params):,}")
        
        # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã®ä½œæˆï¼ˆãƒ¬ã‚¸ã‚¹ãƒˆãƒªçµŒç”±ï¼‰
        optimizer = utils.instantiate(registry.optimizer, self.hparams.optimizer, params)
        print(f"   âœ… ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼: {optimizer.__class__.__name__}")
        
        # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã®è¨­å®šï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        if hasattr(self.hparams, 'scheduler'):
            lr_scheduler = utils.instantiate(registry.scheduler, self.hparams.scheduler, optimizer)
            scheduler = {
                "scheduler": lr_scheduler,
                "interval": self.hparams.train.get("interval", "epoch"),
                "monitor": self.hparams.train.get("monitor", "val/loss"),
                "name": "trainer/lr",
            }
            print(f"   âœ… ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼: {lr_scheduler.__class__.__name__}")
            return [optimizer], [scheduler]
        
        return optimizer
    
    def forward(self, batch):
        """é †æ–¹å‘è¨ˆç®—ï¼ˆä»®å®Ÿè£…ï¼‰"""
        # TODO: ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ãƒ»ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ãƒ»ã‚¿ã‚¹ã‚¯ã®å®Ÿè£…å¾Œã«å®Œå…¨å®Ÿè£…
        x, y, *z = batch
        
        # ç¾åœ¨ã¯ç›´æ¥ãƒ¢ãƒ‡ãƒ«ã‚’å‘¼ã³å‡ºã—
        if hasattr(self.model, 'forward'):
            output, state = self.model(x, state=self._state)
            self._state = state
            return output, y, {}
        else:
            raise NotImplementedError("Model forward method not implemented")
    
    def training_step(self, batch, batch_idx):
        """è¨“ç·´ã‚¹ãƒ†ãƒƒãƒ—ï¼ˆä»®å®Ÿè£…ï¼‰"""
        try:
            x, y, w = self.forward(batch)
            
            # ä»®ã®æå¤±è¨ˆç®—
            loss = torch.nn.functional.mse_loss(x, torch.zeros_like(x))
            
            self.log("train/loss", loss, on_step=True, on_epoch=True, prog_bar=True)
            return loss
        except Exception as e:
            print(f"âš ï¸  Training step error: {e}")
            return torch.tensor(0.0, requires_grad=True)


def setup_logger(cfg: DictConfig) -> WandbLogger:
    """Weights & Biases ãƒ­ã‚¬ãƒ¼ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
    logger = WandbLogger(
        project="ml_pipeline",
        name=f"experiment-{cfg.model.get('_name_', 'unknown')}",
        tags=[],
        offline=True,  # é–‹ç™ºä¸­ã¯ã‚ªãƒ•ãƒ©ã‚¤ãƒ³
    )
    return logger


def setup_callbacks(cfg: DictConfig) -> list:
    """PyTorch Lightning ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ï¼ˆãƒ¬ã‚¸ã‚¹ãƒˆãƒªçµŒç”±ï¼‰"""
    callbacks = []
    
    # ãƒ¢ãƒ‡ãƒ«ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
    checkpoint_config = DictConfig({
        '_name_': 'model_checkpoint',
        'monitor': 'train/loss',
        'mode': 'min',
        'save_top_k': 3,
        'filename': '{epoch:02d}-{train_loss:.3f}',
        'auto_insert_metric_name': False,
    })
    checkpoint_callback = utils.instantiate(registry.callbacks, checkpoint_config)
    callbacks.append(checkpoint_callback)
    
    print(f"   âœ… ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯è¿½åŠ : ModelCheckpoint")
    
    return callbacks


def create_trainer(config, **kwargs):
    """ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã®ä½œæˆï¼ˆå…ƒã®ãƒªãƒã‚¸ãƒˆãƒªã‚¹ã‚¿ã‚¤ãƒ«ï¼‰"""
    callbacks = setup_callbacks(config)
    logger = setup_logger(config)
    
    # GPUè¨­å®šã®è‡ªå‹•èª¿æ•´
    devices = config.trainer.get("devices", 1)
    if devices > 1:
        print(f"[yellow]ğŸ“Ÿ ãƒãƒ«ãƒGPUè¨­å®šæ¤œå‡º: {devices} devices[/yellow]")
        
    trainer = pl.Trainer(
        logger=logger,
        callbacks=callbacks,
        **config.trainer,
        **kwargs,
    )
    return trainer


@hydra.main(version_base=None, config_path="config", config_name="config")
def main(cfg: DictConfig) -> None:
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    
    # è¨­å®šã‚’è¡¨ç¤º
    print("=" * 80)
    print("[bold green]ğŸš€ æ©Ÿæ¢°å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³é–‹å§‹[/bold green]")
    print("=" * 80)
    print("\n[bold blue]ğŸ“‹ è¨­å®š:[/bold blue]")
    print(OmegaConf.to_yaml(cfg))
    
    # ã‚·ãƒ¼ãƒ‰è¨­å®šï¼ˆå†ç¾æ€§ã®ãŸã‚ï¼‰
    if hasattr(cfg.dataset, 'seed'):
        pl.seed_everything(cfg.dataset.seed, workers=True)
        print(f"[green]ğŸ² ã‚·ãƒ¼ãƒ‰è¨­å®š: {cfg.dataset.seed}[/green]")
    
    # ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    os.makedirs("logs", exist_ok=True)
    
    # ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã¨ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ
    print("\n[bold cyan]ğŸ”§ ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–...[/bold cyan]")
    trainer = create_trainer(cfg)
    model = SequenceLightningModule(cfg)
    
    print("\n[bold yellow]âš ï¸  ç¾åœ¨ã®å®Ÿè£…çŠ¶æ³:[/bold yellow]")
    print("  âœ… ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã‚·ã‚¹ãƒ†ãƒ ")
    print("  âœ… åŸºæœ¬æ§‹é€ ã¨Hydraè¨­å®š") 
    print("  âœ… RNNModelå®Ÿè£…")
    print("  âŒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå®Ÿè£…")
    print("  âŒ ã‚¿ã‚¹ã‚¯å®Ÿè£…")
    print("  âŒ ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ãƒ»ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼å®Ÿè£…")
    
    print("\n[bold cyan]ğŸ”§ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:[/bold cyan]")
    print("  1. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå®Ÿè£…")
    print("  2. ã‚¿ã‚¹ã‚¯ï¼ˆæå¤±ãƒ»ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼‰å®Ÿè£…")
    print("  3. ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ãƒ»ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼å®Ÿè£…")
    print("  4. å®Œå…¨ãªè¨“ç·´ãƒ«ãƒ¼ãƒ—")
    
    # è¨­å®šç¢ºèªã¨ãƒ¢ãƒ‡ãƒ«ãƒ†ã‚¹ãƒˆ
    print(f"\n[green]âœ… è¨­å®šç¢ºèªã¨ãƒ¢ãƒ‡ãƒ«ãƒ†ã‚¹ãƒˆå®Œäº†ï¼[/green]")
    
    # TODO: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå®Ÿè£…å¾Œã«æœ‰åŠ¹åŒ–
    """
    # è¨“ç·´å®Ÿè¡Œ
    trainer.fit(model)
    
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    if cfg.train.test:
        trainer.test(model)
    
    print("âœ… è¨“ç·´å®Œäº†!")
    """


if __name__ == "__main__":
    main()
